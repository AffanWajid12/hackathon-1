{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install SpeechRecognition transformers pydub librosa\n",
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg -y\n",
        "\n",
        "from google.colab import output\n",
        "from IPython.display import HTML, Javascript, display\n",
        "from transformers import pipeline\n",
        "import speech_recognition as sr\n",
        "import numpy as np\n",
        "import base64\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import io\n",
        "\n",
        "# Initialize components with advanced emotion detection model\n",
        "emotion_analyzer = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"SamLowe/roberta-base-go_emotions\",  # More comprehensive emotion model\n",
        "    top_k=None  # Get all emotion scores\n",
        ")\n",
        "\n",
        "# Emotion mapping to simplify outputs\n",
        "EMOTION_MAPPING = {\n",
        "    'admiration': 'Admiration',\n",
        "    'amusement': 'Amusement',\n",
        "    'anger': 'Anger',\n",
        "    'annoyance': 'Annoyance',\n",
        "    'approval': 'Approval',\n",
        "    'caring': 'Caring',\n",
        "    'confusion': 'Confusion',\n",
        "    'curiosity': 'Curiosity',\n",
        "    'desire': 'Desire',\n",
        "    'disappointment': 'Disappointment',\n",
        "    'disapproval': 'Disapproval',\n",
        "    'disgust': 'Disgust',\n",
        "    'embarrassment': 'Embarrassment',\n",
        "    'excitement': 'Excitement',\n",
        "    'fear': 'Fear',\n",
        "    'gratitude': 'Gratitude',\n",
        "    'grief': 'Grief',\n",
        "    'joy': 'Joy',\n",
        "    'love': 'Love',\n",
        "    'nervousness': 'Nervousness',\n",
        "    'optimism': 'Optimism',\n",
        "    'pride': 'Pride',\n",
        "    'realization': 'Realization',\n",
        "    'relief': 'Relief',\n",
        "    'remorse': 'Remorse',\n",
        "    'sadness': 'Sadness',\n",
        "    'surprise': 'Surprise',\n",
        "    'neutral': 'Neutral'\n",
        "}\n",
        "\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Configure recognizer for better accuracy\n",
        "recognizer.energy_threshold = 4000\n",
        "recognizer.dynamic_energy_threshold = True\n",
        "recognizer.pause_threshold = 0.8\n",
        "\n",
        "# HTML interface with enhanced audio processing\n",
        "display(HTML(\"\"\"\n",
        "<div id=\"recorder\">\n",
        "  <button id=\"recordBtn\" style=\"padding: 10px 20px; font-size: 16px;\">Start Recording</button>\n",
        "  <p id=\"status\" style=\"font-weight: bold; color: #333;\">Ready to record</p>\n",
        "  <div id=\"results\" style=\"margin-top: 20px;\"></div>\n",
        "</div>\n",
        "\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/recordrtc@5.6.2/RecordRTC.min.js\"></script>\n",
        "<script>\n",
        "let recorder;\n",
        "let audioChunks = [];\n",
        "\n",
        "async function startRecording() {\n",
        "  try {\n",
        "    const stream = await navigator.mediaDevices.getUserMedia({\n",
        "      audio: {\n",
        "        echoCancellation: true,\n",
        "        noiseSuppression: true,\n",
        "        sampleRate: 44100\n",
        "      }\n",
        "    });\n",
        "\n",
        "    recorder = RecordRTC(stream, {\n",
        "      type: 'audio',\n",
        "      mimeType: 'audio/wav',\n",
        "      sampleRate: 44100,\n",
        "      desiredSampRate: 44100,\n",
        "      recorderType: RecordRTC.StereoAudioRecorder,\n",
        "      numberOfAudioChannels: 1,\n",
        "      timeSlice: 250,\n",
        "      ondataavailable: blob => {\n",
        "        audioChunks.push(blob);\n",
        "      }\n",
        "    });\n",
        "\n",
        "    recorder.startRecording();\n",
        "    document.getElementById('recordBtn').textContent = 'Stop & Analyze';\n",
        "    document.getElementById('status').textContent = 'Recording... Speak clearly now!';\n",
        "    document.getElementById('status').style.color = '#d35400';\n",
        "    return true;\n",
        "  } catch (err) {\n",
        "    document.getElementById('status').textContent = 'Error: ' + err.message;\n",
        "    document.getElementById('status').style.color = '#c0392b';\n",
        "    return false;\n",
        "  }\n",
        "}\n",
        "\n",
        "async function stopRecording() {\n",
        "  return new Promise(resolve => {\n",
        "    recorder.stopRecording(() => {\n",
        "      const blob = recorder.getBlob();\n",
        "      const reader = new FileReader();\n",
        "\n",
        "      reader.onload = function() {\n",
        "        const base64data = this.result.split(',')[1];\n",
        "        resolve(base64data);\n",
        "      };\n",
        "\n",
        "      reader.readAsDataURL(blob);\n",
        "      document.getElementById('status').textContent = 'Processing audio...';\n",
        "      document.getElementById('status').style.color = '#16a085';\n",
        "\n",
        "      // Stop all tracks\n",
        "      recorder.getDataURL().getTracks().forEach(track => track.stop());\n",
        "    });\n",
        "  });\n",
        "}\n",
        "\n",
        "document.getElementById('recordBtn').onclick = async function() {\n",
        "  if (this.textContent === 'Start Recording') {\n",
        "    await startRecording();\n",
        "  } else {\n",
        "    const audioData = await stopRecording();\n",
        "    google.colab.kernel.invokeFunction('analyzeAudio', [audioData], {});\n",
        "    this.textContent = 'Start Recording';\n",
        "  }\n",
        "};\n",
        "</script>\n",
        "\"\"\"))\n",
        "\n",
        "def enhance_audio(audio_bytes):\n",
        "    \"\"\"Improve audio quality using librosa\"\"\"\n",
        "    audio, sr = librosa.load(io.BytesIO(audio_bytes), sr=44100)\n",
        "    audio = librosa.effects.preemphasis(audio)\n",
        "    audio = librosa.util.normalize(audio)\n",
        "    buffer = io.BytesIO()\n",
        "    sf.write(buffer, audio, sr, format='WAV')\n",
        "    return buffer.getvalue()\n",
        "\n",
        "def analyze_audio(audio_base64):\n",
        "    try:\n",
        "        # Convert and enhance audio\n",
        "        audio_bytes = base64.b64decode(audio_base64)\n",
        "        enhanced_audio = enhance_audio(audio_bytes)\n",
        "        audio_array = np.frombuffer(enhanced_audio, dtype=np.int16)\n",
        "        audio_data = sr.AudioData(audio_array, 44100, 2)\n",
        "\n",
        "        # Recognize speech\n",
        "        try:\n",
        "            text = recognizer.recognize_google(audio_data, language='en-US')\n",
        "        except:\n",
        "            text = recognizer.recognize_whisper(audio_data, model='base.en')\n",
        "\n",
        "        print(f\"\\n🎤 Recognized text: {text}\")\n",
        "\n",
        "        # Analyze emotions (returns all emotion scores)\n",
        "        raw_results = emotion_analyzer(text)[0]\n",
        "\n",
        "        # Process and map emotions\n",
        "        results = []\n",
        "        for result in raw_results:\n",
        "            label = result['label']\n",
        "            if label in EMOTION_MAPPING:\n",
        "                results.append({\n",
        "                    'label': EMOTION_MAPPING[label],\n",
        "                    'score': result['score']\n",
        "                })\n",
        "\n",
        "        # Sort by score (highest first)\n",
        "        results.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        # Get top 3 emotions\n",
        "        top_emotions = results[:3]\n",
        "        dominant_emotion = top_emotions[0]\n",
        "\n",
        "        # Format results for display\n",
        "        emotion_results = \"\\n\".join([\n",
        "            f\"• {result['label']}: {result['score']:.2%}\"\n",
        "            for result in results[:5]  # Show top 5 emotions\n",
        "        ])\n",
        "\n",
        "        print(f\"\\n🎭 Detected Emotions:\\n{emotion_results}\")\n",
        "        print(f\"\\n🌟 Dominant Emotion: {dominant_emotion['label']} ({dominant_emotion['score']:.2%} confidence)\")\n",
        "\n",
        "        # Prepare data for JavaScript visualization\n",
        "        results_js = [\n",
        "            {'label': r['label'], 'score': round(r['score'] * 100)}\n",
        "            for r in results[:5]  # Show top 5 emotions in visualization\n",
        "        ]\n",
        "\n",
        "        # Determine emotion color\n",
        "        emotion_colors = {\n",
        "            'Positive': '#2ecc71',\n",
        "            'Negative': '#e74c3c',\n",
        "            'Neutral': '#3498db'\n",
        "        }\n",
        "\n",
        "        # Categorize dominant emotion\n",
        "        if dominant_emotion['label'] in ['Joy', 'Love', 'Excitement', 'Gratitude', 'Amusement', 'Pride']:\n",
        "            emotion_type = 'Positive'\n",
        "        elif dominant_emotion['label'] in ['Anger', 'Sadness', 'Fear', 'Disgust', 'Grief', 'Remorse']:\n",
        "            emotion_type = 'Negative'\n",
        "        else:\n",
        "            emotion_type = 'Neutral'\n",
        "\n",
        "        # Visual feedback with emotion display\n",
        "        js_code = f\"\"\"\n",
        "        document.getElementById('status').textContent = 'Analysis complete!';\n",
        "        document.getElementById('status').style.color = '#27ae60';\n",
        "\n",
        "        // Create HTML for results\n",
        "        const resultsHTML = `\n",
        "            <div style=\"background: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 10px;\">\n",
        "                <h3 style=\"margin-top: 0;\">Emotion Analysis Results</h3>\n",
        "\n",
        "                <div style=\"display: flex; align-items: center; margin-bottom: 15px;\">\n",
        "                    <div style=\"font-size: 24px; margin-right: 15px;\">\n",
        "                        <strong>Final Emotion:</strong>\n",
        "                        <span style=\"color: {emotion_colors[emotion_type]}\">\n",
        "                            {dominant_emotion['label']} ({round(dominant_emotion['score']*100)}%)\n",
        "                        </span>\n",
        "                    </div>\n",
        "                    <div style=\"background: #e9ecef; height: 10px; flex-grow: 1; border-radius: 5px;\">\n",
        "                        <div style=\"background: {emotion_colors[emotion_type]}; width: {dominant_emotion['score']*100}%; height: 100%; border-radius: 5px;\"></div>\n",
        "                    </div>\n",
        "                </div>\n",
        "\n",
        "                <p><strong>Emotion Breakdown:</strong></p>\n",
        "                <ul style=\"padding-left: 20px; margin-bottom: 15px;\">\n",
        "                    {''.join([f\"<li>{r['label']}: {r['score']}%</li>\" for r in results_js])}\n",
        "                </ul>\n",
        "\n",
        "                <div style=\"background: #f1f1f1; padding: 10px; border-radius: 5px;\">\n",
        "                    <p style=\"margin: 0;\"><strong>Recognized Text:</strong> {text}</p>\n",
        "                </div>\n",
        "            </div>\n",
        "        `;\n",
        "\n",
        "        document.getElementById('results').innerHTML = resultsHTML;\n",
        "        \"\"\"\n",
        "\n",
        "        display(Javascript(js_code))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error: {str(e)}\")\n",
        "        display(Javascript(\"\"\"\n",
        "        document.getElementById('status').textContent = 'Error in analysis';\n",
        "        document.getElementById('status').style.color = '#c0392b';\n",
        "        document.getElementById('results').innerHTML = '';\n",
        "        \"\"\"))\n",
        "\n",
        "# Register the callback\n",
        "output.register_callback('analyzeAudio', analyze_audio)"
      ],
      "metadata": {
        "id": "c2atd1ixBHSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70ee1c6f-553e-420f-ddc3-71c08cec1364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libportaudio2 is already the newest version (19.6.0-1.1).\n",
            "libportaudiocpp0 is already the newest version (19.6.0-1.1).\n",
            "portaudio19-dev is already the newest version (19.6.0-1.1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div id=\"recorder\">\n",
              "  <button id=\"recordBtn\" style=\"padding: 10px 20px; font-size: 16px;\">Start Recording</button>\n",
              "  <p id=\"status\" style=\"font-weight: bold; color: #333;\">Ready to record</p>\n",
              "  <div id=\"results\" style=\"margin-top: 20px;\"></div>\n",
              "</div>\n",
              "\n",
              "<script src=\"https://cdn.jsdelivr.net/npm/recordrtc@5.6.2/RecordRTC.min.js\"></script>\n",
              "<script>\n",
              "let recorder;\n",
              "let audioChunks = [];\n",
              "\n",
              "async function startRecording() {\n",
              "  try {\n",
              "    const stream = await navigator.mediaDevices.getUserMedia({ \n",
              "      audio: {\n",
              "        echoCancellation: true,\n",
              "        noiseSuppression: true,\n",
              "        sampleRate: 44100\n",
              "      } \n",
              "    });\n",
              "    \n",
              "    recorder = RecordRTC(stream, {\n",
              "      type: 'audio',\n",
              "      mimeType: 'audio/wav',\n",
              "      sampleRate: 44100,\n",
              "      desiredSampRate: 44100,\n",
              "      recorderType: RecordRTC.StereoAudioRecorder,\n",
              "      numberOfAudioChannels: 1,\n",
              "      timeSlice: 250,\n",
              "      ondataavailable: blob => {\n",
              "        audioChunks.push(blob);\n",
              "      }\n",
              "    });\n",
              "    \n",
              "    recorder.startRecording();\n",
              "    document.getElementById('recordBtn').textContent = 'Stop & Analyze';\n",
              "    document.getElementById('status').textContent = 'Recording... Speak clearly now!';\n",
              "    document.getElementById('status').style.color = '#d35400';\n",
              "    return true;\n",
              "  } catch (err) {\n",
              "    document.getElementById('status').textContent = 'Error: ' + err.message;\n",
              "    document.getElementById('status').style.color = '#c0392b';\n",
              "    return false;\n",
              "  }\n",
              "}\n",
              "\n",
              "async function stopRecording() {\n",
              "  return new Promise(resolve => {\n",
              "    recorder.stopRecording(() => {\n",
              "      const blob = recorder.getBlob();\n",
              "      const reader = new FileReader();\n",
              "      \n",
              "      reader.onload = function() {\n",
              "        const base64data = this.result.split(',')[1];\n",
              "        resolve(base64data);\n",
              "      };\n",
              "      \n",
              "      reader.readAsDataURL(blob);\n",
              "      document.getElementById('status').textContent = 'Processing audio...';\n",
              "      document.getElementById('status').style.color = '#16a085';\n",
              "      \n",
              "      // Stop all tracks\n",
              "      recorder.getDataURL().getTracks().forEach(track => track.stop());\n",
              "    });\n",
              "  });\n",
              "}\n",
              "\n",
              "document.getElementById('recordBtn').onclick = async function() {\n",
              "  if (this.textContent === 'Start Recording') {\n",
              "    await startRecording();\n",
              "  } else {\n",
              "    const audioData = await stopRecording();\n",
              "    google.colab.kernel.invokeFunction('analyzeAudio', [audioData], {});\n",
              "    this.textContent = 'Start Recording';\n",
              "  }\n",
              "};\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎤 Recognized text: I am feeling very tired today\n",
            "\n",
            "🎭 Detected Emotions:\n",
            "• Sadness: 49.79%\n",
            "• Neutral: 20.37%\n",
            "• Disappointment: 14.84%\n",
            "• Annoyance: 3.68%\n",
            "• Approval: 2.11%\n",
            "\n",
            "🌟 Dominant Emotion: Sadness (49.79% confidence)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        document.getElementById('status').textContent = 'Analysis complete!';\n",
              "        document.getElementById('status').style.color = '#27ae60';\n",
              "        \n",
              "        // Create HTML for results\n",
              "        const resultsHTML = `\n",
              "            <div style=\"background: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 10px;\">\n",
              "                <h3 style=\"margin-top: 0;\">Emotion Analysis Results</h3>\n",
              "                \n",
              "                <div style=\"display: flex; align-items: center; margin-bottom: 15px;\">\n",
              "                    <div style=\"font-size: 24px; margin-right: 15px;\">\n",
              "                        <strong>Final Emotion:</strong> \n",
              "                        <span style=\"color: #e74c3c\">\n",
              "                            Sadness (50%)\n",
              "                        </span>\n",
              "                    </div>\n",
              "                    <div style=\"background: #e9ecef; height: 10px; flex-grow: 1; border-radius: 5px;\">\n",
              "                        <div style=\"background: #e74c3c; width: 49.791234731674194%; height: 100%; border-radius: 5px;\"></div>\n",
              "                    </div>\n",
              "                </div>\n",
              "                \n",
              "                <p><strong>Emotion Breakdown:</strong></p>\n",
              "                <ul style=\"padding-left: 20px; margin-bottom: 15px;\">\n",
              "                    <li>Sadness: 50%</li><li>Neutral: 20%</li><li>Disappointment: 15%</li><li>Annoyance: 4%</li><li>Approval: 2%</li>\n",
              "                </ul>\n",
              "                \n",
              "                <div style=\"background: #f1f1f1; padding: 10px; border-radius: 5px;\">\n",
              "                    <p style=\"margin: 0;\"><strong>Recognized Text:</strong> I am feeling very tired today</p>\n",
              "                </div>\n",
              "            </div>\n",
              "        `;\n",
              "        \n",
              "        document.getElementById('results').innerHTML = resultsHTML;\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎤 Recognized text: I am feeling very tired today\n",
            "\n",
            "🎭 Detected Emotions:\n",
            "• Sadness: 49.79%\n",
            "• Neutral: 20.37%\n",
            "• Disappointment: 14.84%\n",
            "• Annoyance: 3.68%\n",
            "• Approval: 2.11%\n",
            "\n",
            "🌟 Dominant Emotion: Sadness (49.79% confidence)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        document.getElementById('status').textContent = 'Analysis complete!';\n",
              "        document.getElementById('status').style.color = '#27ae60';\n",
              "        \n",
              "        // Create HTML for results\n",
              "        const resultsHTML = `\n",
              "            <div style=\"background: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 10px;\">\n",
              "                <h3 style=\"margin-top: 0;\">Emotion Analysis Results</h3>\n",
              "                \n",
              "                <div style=\"display: flex; align-items: center; margin-bottom: 15px;\">\n",
              "                    <div style=\"font-size: 24px; margin-right: 15px;\">\n",
              "                        <strong>Final Emotion:</strong> \n",
              "                        <span style=\"color: #e74c3c\">\n",
              "                            Sadness (50%)\n",
              "                        </span>\n",
              "                    </div>\n",
              "                    <div style=\"background: #e9ecef; height: 10px; flex-grow: 1; border-radius: 5px;\">\n",
              "                        <div style=\"background: #e74c3c; width: 49.791234731674194%; height: 100%; border-radius: 5px;\"></div>\n",
              "                    </div>\n",
              "                </div>\n",
              "                \n",
              "                <p><strong>Emotion Breakdown:</strong></p>\n",
              "                <ul style=\"padding-left: 20px; margin-bottom: 15px;\">\n",
              "                    <li>Sadness: 50%</li><li>Neutral: 20%</li><li>Disappointment: 15%</li><li>Annoyance: 4%</li><li>Approval: 2%</li>\n",
              "                </ul>\n",
              "                \n",
              "                <div style=\"background: #f1f1f1; padding: 10px; border-radius: 5px;\">\n",
              "                    <p style=\"margin: 0;\"><strong>Recognized Text:</strong> I am feeling very tired today</p>\n",
              "                </div>\n",
              "            </div>\n",
              "        `;\n",
              "        \n",
              "        document.getElementById('results').innerHTML = resultsHTML;\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rf68HzQ8Cbhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}